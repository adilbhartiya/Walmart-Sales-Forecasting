{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30753b16",
   "metadata": {},
   "source": [
    "## Step:1 Dataset Description and Objective:\n",
    "\n",
    "**Dataset Description:** The dataset for this project, walmart.csv, represents weekly sales data from various Walmart outlets across the country. These retail stores, which have multiple outlets nationwide, are facing challenges in managing their inventory to effectively match supply with demand. The dataset contains 6435 rows and 8 columns. The columns represent the following features:\n",
    "\n",
    "Store: The store number         \n",
    "Date: The week of sales          \n",
    "Weekly_Sales: The sales for the given store in that week          \n",
    "Holiday_Flag: A flag indicating if it is a holiday week          \n",
    "Temperature: The temperature on the day of the sale         \n",
    "Fuel_Price: The cost of fuel in the region           \n",
    "CPI: The Consumer Price Index          \n",
    "Unemployment: The Unemployment Rate            \n",
    "\n",
    "**Objective of the EDA:** The objective of the Exploratory Data Analysis (EDA) for this project is to gain insights into the factors affecting the weekly sales of the Walmart stores. The EDA will focus on understanding the impact of various factors such as unemployment rate, seasonal trends, temperature, and Consumer Price Index on the weekly sales. The EDA will also identify the top and worst performing stores based on the historical sales data. These insights will then be used to forecast the sales for each store for the next 12 weeks using predictive modeling techniques. The ultimate goal is to help Walmart manage its inventory more effectively by matching the demand with respect to supply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa8c86",
   "metadata": {},
   "source": [
    "## Step:2 Import Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30420a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "# Data visualization libraries\n",
    "!pip install plotly\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Ignore UserWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Library for Model Saving and Loading\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d706b",
   "metadata": {},
   "source": [
    "## Step:3 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"E:\\csv\\Intel csv\\Walmart DataSet.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35417a2f",
   "metadata": {},
   "source": [
    "## Step:4 Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b6bca",
   "metadata": {},
   "source": [
    "### 1) Data Overview\n",
    "* Checking the dimensions of the dataset (number of rows and columns).\n",
    "* Inspect first few rows to understand the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194fc533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display last 5 rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ac1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shape of dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50d300",
   "metadata": {},
   "source": [
    "* In this dataset we have 6435 rows and 8 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88361904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It helps to understand the data type and information about data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1f8af",
   "metadata": {},
   "source": [
    "From the info() method, we can deduce that our dataset encompasses three different data types: int64, float64, and object.\n",
    "\n",
    "**Datatype of column:**\n",
    "* The Store and Holiday_Flag columns are of type int64.          \n",
    "* The Date column is categorized as object, typically indicative of string or categorical data. \n",
    "    * If the Date column represents dates, it might be advantageous to convert it to a datetime data type for facilitating date-related operations.        \n",
    "* The remaining columns have a data type of float64.           \n",
    "\n",
    "**About missing values:**\n",
    "* By employing this method, we can effortlessly detect the presence of any missing values. \n",
    "* Here since each column contains 6435 observations, corresponding to the identical number of rows we previously observed using the shape attribute. So we can conclude that none of the column has missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61320c8",
   "metadata": {},
   "source": [
    "### 2) Checking for Duplicates:\n",
    "* Identify and remove duplicate rows in the dataset to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Duplicates Values\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51185c",
   "metadata": {},
   "source": [
    "* Sum is 0, meaning there are no duplicate rows in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c4378",
   "metadata": {},
   "source": [
    "### 3) Summary Statisitc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c6bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The describe method Compute descriptive statistics for numerical variables like min,max etc\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491fa930",
   "metadata": {},
   "source": [
    "* Store Distribution: There are 45 unique stores represented in the dataset, ranging from Store 1 to Store 45. The distribution of data across stores is relatively balanced, with each store having a similar number of observations on average.\n",
    "\n",
    "* Weekly Sales Distribution: The average weekly sales across all stores is approximately $1,046,965. The minimum and maximum weekly sales are approximately $209,986 and $3,818,686 respectively. This indicates a significant variation in weekly sales.\n",
    "\n",
    "* Holiday Flag Distribution: Approximately 7% of the data points are from holiday weeks.\n",
    "\n",
    "* Temperature Distribution: The average temperature is about 60.66°F, with a minimum of -2.06°F and a maximum of 100.14°F. The distribution of temperatures appears to be relatively normal.\n",
    "\n",
    "* Fuel Price Distribution: The average fuel price is approximately $3.36 per gallon, with prices ranging from around $2.47 to $4.47 per gallon. The distribution of fuel prices appears to be relatively normal.\n",
    "\n",
    "* Consumer Price Index (CPI) Distribution: The average CPI is approximately 171.58, with values ranging from approximately 126.06 to 227.23. The distribution of CPI values may vary across different regions and time periods.\n",
    "\n",
    "* Unemployment Rate Distribution: The average unemployment rate is approximately 8%, with rates ranging from approximately 3.87% to 14.31%. The distribution of unemployment rates may reflect economic conditions and regional variations.\n",
    "\n",
    "Overall, these insights provide a comprehensive overview of the distribution and variability of each feature in the dataset. Further analysis, such as correlation analysis and visualization, can help uncover additional patterns and relationships within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency counts for the 'Store' column\n",
    "store_counts = df['Store'].value_counts()\n",
    "store_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92cec5",
   "metadata": {},
   "source": [
    "* So from above data we can tell that each store have 143 records, suggesting consistent data collection across all locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency counts for the 'Holiday_Flag' column\n",
    "holiday_counts = df['Holiday_Flag'].value_counts()\n",
    "holiday_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe70c42",
   "metadata": {},
   "source": [
    "* There are 5985 instances (or weeks) where Holiday_Flag is 0, indicating regular weeks without any holidays. \n",
    "* Additionally, there are 450 instances where Holiday_Flag is 1, signifying these weeks include at least one holiday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86cd62b",
   "metadata": {},
   "source": [
    "### 4) Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if any column having null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa1295",
   "metadata": {},
   "source": [
    "* Previously, we determined that our dataset has no missing values by using the info() method. We reverified this by using the isna() function, confirming that our dataset does not contain any null values.\n",
    "* We also checked for any irregularities in our data and found none."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add62a38",
   "metadata": {},
   "source": [
    "### 5) Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23e5dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "col = df.columns.values\n",
    "\n",
    "for col_name in col:\n",
    "    if df[col_name].dtype != 'object':\n",
    "        fig = px.box(df, y=col_name)\n",
    "        fig.update_layout(title=f'Box plot of {col_name}', xaxis_title=col_name, yaxis_title='Count')\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e08f40d",
   "metadata": {},
   "source": [
    " **Note:1**\n",
    "\n",
    "* For categorical variables with only two unique values (often represented as 1 and 0) like here in 'Holiday_Flag', the concept of outliers doesn't apply in the traditional sense. Outliers are typically defined as data points that significantly deviate from the rest of the observations in a numerical distribution.\n",
    "* so we have outliers in 'Weekly_sales', 'Temperature' and 'Unemployment' columns\n",
    "\n",
    " **Note:2**\n",
    "* In many cases, 'Weekly_Sales' is considered as the target or dependent variable in predictive modeling tasks, such as sales forecasting.\n",
    "* Removing outliers from the target variable may lead to biased model predictions. \n",
    "* If outliers represent genuine data points (e.g., exceptionally high sales during holiday seasons), removing them can distort the model's ability to capture such patterns.\n",
    "* However, it's essential to preprocess other independent variables (such as 'Temperature' and 'Unemployment') to mitigate the impact of outliers on model training and performance.\n",
    "\n",
    " **Note:3**\n",
    "* Here ‘Temperature’ and ‘Unemployment’ columns have outliers, it could be due to natural fluctuations in weather and economic conditions. \n",
    "* In such cases, these outliers are actual representations of the variability in the data, and removing them might lead to loss of information.\n",
    "* I have checked before if we remove these outliers \n",
    "    * So before removing outlier, we have 6435 rows\n",
    "    * but after removing outlier we got only 5951 rows, \n",
    "    * means we have removed 484 rows and i.e. nothing but loss of 7.5% of data and i.e not a small amount. which could potentially include important, meaningful data. \n",
    "* Therefore i am thinking filling data with median is good way to resolve this issue.\n",
    "\n",
    "Imputation: Replace the outlier values with statistical measures such as mean, median or mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Define column names\n",
    "col_names = ['Temperature', 'Unemployment']\n",
    "\n",
    "# Iterate over each column\n",
    "for col_name in col_names:\n",
    "    # Calculate quartiles and IQR\n",
    "    Q1 = df[col_name].quantile(0.25)\n",
    "    Q3 = df[col_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define lower and upper bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Replace outliers with the median of the column\n",
    "    df[col_name] = df[col_name].apply(lambda x: df[col_name].median() if (x < lower_bound or x > upper_bound) else x)\n",
    "\n",
    "# Create box plots after replacing outliers\n",
    "for col_name in col_names:\n",
    "    fig = px.box(df, y=col_name)\n",
    "    fig.update_layout(title=f'Box plot of {col_name} (Outliers Replaced)', xaxis_title=col_name, yaxis_title='Value')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf8b52",
   "metadata": {},
   "source": [
    "### 6) Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eebdf5",
   "metadata": {},
   "source": [
    "#### 6.1) Examine the distribution of Variables using histograms\n",
    "\n",
    "* These columns are particularly relevant for understanding the factors that influence weekly sales and overall store performance. By visualizing their distributions through histograms, we can gain insights into the variability, central tendency, and potential relationships within the data, aiding in the analysis and decision-making process for inventory management strategies.\n",
    "\n",
    "* Weekly_Sales: This is the target variable, and understanding its distribution is crucial for analyzing sales patterns across different stores and time periods. A histogram of weekly sales can provide insights into the range of sales values, the presence of outliers, and the overall distribution shape.\n",
    "\n",
    "* Temperature: Analyzing the distribution of temperature values can help identify seasonal patterns and understand how temperature variations might impact sales of certain products, such as seasonal items or weather-dependent goods.\n",
    "\n",
    "* Fuel_Price: Examining the distribution of fuel prices can reveal trends in fuel costs over time and their potential impact on transportation expenses, which may influence sales patterns and store profitability.\n",
    "\n",
    "* CPI (Consumer Price Index): Understanding the distribution of CPI values can provide insights into regional economic conditions and consumer spending power, which may affect purchasing behavior and sales trends.\n",
    "\n",
    "* Unemployment: Analyzing the distribution of unemployment rates can help assess the economic environment in different regions and understand how employment trends might influence consumer confidence and spending patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e623b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Weekly_Sales\n",
    "fig = px.histogram(df, x = df['Weekly_Sales'], marginal='box', title = 'Distribution of Weekly_Sales', nbins = 50)\n",
    "fig.update_layout(xaxis_title = 'Weekly_Sales', yaxis_title = 'Frequency')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca6179",
   "metadata": {},
   "source": [
    "* The distribution of weekly sales is right-skewed, meaning there are a few weeks with exceptionally high sales.\n",
    "* The majority of the weekly sales values fall within the box in the box plot, while there are some outliers represented as individual points above the box.\n",
    "* The most common range of weekly sales is around 0.5M, as indicated by the highest bar in the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58ad13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution of Temperature\n",
    "fig = px.histogram(df, x = df['Temperature'],marginal='box', title = 'Distribution of Temperature', nbins = 50)\n",
    "fig.update_layout(xaxis_title = 'Temperature', yaxis_title = 'Frequency')\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88113e5",
   "metadata": {},
   "source": [
    "* The shape of the distribution appears to be somewhat bell-shaped, peaking around a temperature of 60. This suggests that most of the temperature values in your dataset are around this value.\n",
    "* There are fewer occurrences of very low and very high temperatures, as indicated by the lower bars at the ends of the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a1357",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution of Fuel_Price\n",
    "fig = px.histogram(df, x = df['Fuel_Price'],marginal='box', title = 'Distribution of Fuel_Price', nbins = 50)\n",
    "fig.update_layout(xaxis_title = 'Fuel_Price', yaxis_title = 'Frequency')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328fed6",
   "metadata": {},
   "source": [
    "* The shape of the distribution appears to be somewhat bell-shaped, peaking around a fuel price of 3.75 to 3.799. This suggests that most of the fuel prices in your dataset are around this value.\n",
    "* The histogram shows that the frequency of fuel prices between 2.5 and 3.5 is relatively lower compared to the peak around 3.75 to 3.799.\n",
    "* There are fewer occurrences of very low and very high fuel prices, as indicated by the lower bars at the ends of the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965fa34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of CPI\n",
    "fig = px.histogram(df, x = df['CPI'], marginal='box', title = 'Distribution of CPI', nbins = 50)\n",
    "fig.update_layout(xaxis_title = 'CPI', yaxis_title = 'Frequency')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2466a",
   "metadata": {},
   "source": [
    "* The shape of the distribution appears to be bimodal, with two peaks: one around a CPI of 140 and another just above 200. \n",
    "* This suggests that most of the CPI values in your dataset are around these two values.\n",
    "* There are fewer occurrences of very low and very high CPI values, as indicated by the lower bars at the ends of the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Unemployment\n",
    "fig = px.histogram(df, x = df['Unemployment'], marginal='box', title = 'Distribution of Unemployment', nbins = 50)\n",
    "fig.update_layout(xaxis_title = 'Unemployment', yaxis_title = 'Frequency')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee08c09",
   "metadata": {},
   "source": [
    "* The shape of the distribution appears to be somewhat bell-shaped, peaking around an unemployment rate of around 8. This suggests that most of the unemployment rates in your dataset are around this value.\n",
    "* There are fewer occurrences of very low and very high unemployment rates, as indicated by the lower bars at the ends of the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b2675",
   "metadata": {},
   "source": [
    "    In summary:\n",
    "* Weekly_Sales is right-skewed\n",
    "* Temperature, Fuel Price, and Unemployment Rate appear to be somewhat normally distributed\n",
    "* CPI appears to be bimodal, indicating that there are two distinct peaks or modes in the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855aa280",
   "metadata": {},
   "source": [
    "#### 6.2) Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33647184",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc14732",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fbb41f",
   "metadata": {},
   "source": [
    "* During info() method we have seen 'Date' column is of object type, !!!Right\n",
    "* but as we are planning to perform forecasting, it is important to have our ‘Date’ column in a standard date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1644f19",
   "metadata": {},
   "source": [
    "So now, we have `one column of 'datetime' data type,` and `the remaining columns are either of 'int64' or 'float64' data types`, which represent numerical values. Therefore, there is no need for label or one-hot encoding to convert categorical variables into numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42296642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba92c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "def get_season(month):\n",
    "    # Define North American seasons based on month ranges\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "# Use the 'assign' method to add multiple columns in a single line\n",
    "df = df.assign(\n",
    "    Day=df['Date'].dt.day,  # Add a day column\n",
    "    Day_Of_Week=df['Date'].dt.day_name(),  # Add a day_of_week column\n",
    "    Week=df['Date'].dt.isocalendar().week,  # Add a week column\n",
    "    Month=df['Date'].dt.month,  # Add a month column\n",
    "    Month_Name=df['Date'].dt.month_name(),  # Add a month_name column\n",
    "    Quarter=df['Date'].dt.quarter,  # Add a quarter column (q1, q2, q3 and q3)\n",
    "    Season=df['Date'].dt.month.apply(get_season),  # Assign seasons based on month\n",
    "    Year=df['Date'].dt.year  # Add a year column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c503e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea46141",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18927108",
   "metadata": {},
   "source": [
    "#### 6.3) Analyzing categorical variable\n",
    "* Categorical Features such as Holiday_Flag, Day_Of_Week, Month_Name, Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99922269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weekly sales between holidays and non-holidays\n",
    "fig = px.box(df, x='Holiday_Flag', y='Weekly_Sales', title='Weekly Sales by Holiday', width = 800, height = 500)\n",
    "fig.update_layout(xaxis=dict(tickmode='linear', tickvals=[0, 1], ticktext=['Normal Day', 'Holiday']))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3a26d",
   "metadata": {},
   "source": [
    "* Avergae sale are higher during holidays as compared to non-holiday days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart\n",
    "labels = ['Not Holiday', 'Holiday']\n",
    "values = df['Holiday_Flag'].value_counts()\n",
    "colors = ['#FF6692', '#FFDBC0']  # Using predefined color names\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='percent', marker=dict(colors=colors))])\n",
    "fig.update_layout(title='Distribution of Holiday Flag')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d739e9",
   "metadata": {},
   "source": [
    "* By this pie chart we can cleary see 'Dominance of Non-Holiday Sales' means bulk of sales occurs during non-holiday periods\n",
    "* But 6.99% sales done during holiday smaller in percentage, yet potentially significant, increase in sales during holidays. This could be due to seasonal promotions or holiday shopping trends. \n",
    "* Strategic Planning: The distribution suggests that while holiday periods are important, the store should focus on optimizing sales throughout the year, as non-holiday periods contribute more to the overall sales volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Observations by Year:\n",
    "year_counts = df['Year'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=year_counts.index.astype(str),  # Years\n",
    "    y=year_counts.values, # Counts\n",
    "    text=year_counts.values,  # Text displayed on each bar\n",
    "    textposition='auto',  # Automatically position text\n",
    "    marker=dict(color=['#F08080', '#ADD8E6', '#90EE90'])  # Color palette\n",
    ")])\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Count of Observations by Year',\n",
    "    xaxis=dict(title='Year'),\n",
    "    yaxis=dict(title='Count'),\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Observations by Year:\n",
    "year_counts = df['Year'].value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "fig = go.Figure(data=[go.Pie(labels=year_counts.index, values=year_counts, textinfo='percent', marker=dict(colors=['#F08080', '#ADD8E6', '#90EE90']))])\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(title='Distribution of Observations by Year')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6bdc5",
   "metadata": {},
   "source": [
    "* The bar plot and pie chart illustrates an even distribution of observations across 2010, 2011, and 2012. \n",
    "* Though 2011 shows a slight increase, overall consistency suggests stable trends over the three years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Observations by Season:\n",
    "season_counts = df['Season'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=season_counts.index,  # Seasons\n",
    "    y=season_counts.values,  # Counts\n",
    "    text=season_counts.values,  # Text displayed on each bar\n",
    "    textposition='auto',  # Automatically position text\n",
    "    marker=dict(color=['#FA6E1B', '#008000','#FFA500', '#A6D1F3'])  # Color palette\n",
    ")])\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Count of Observations by Season',\n",
    "    xaxis=dict(title='Season'),\n",
    "    yaxis=dict(title='Count'),\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de46a40",
   "metadata": {},
   "source": [
    "* Summer has the highest count of observations, suggesting that it might be the busiest season.\n",
    "* Spring follows closely behind Summer in terms of the count of observations.\n",
    "* Fall and Winter have fewer observations, indicating that these seasons might be less busy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbcef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Observations by Season:\n",
    "season_counts = df['Season'].value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "fig = go.Figure(data=[go.Pie(labels=season_counts.index, values=season_counts, textinfo='percent', marker=dict(colors=['#FA6E1B', '#008000','#FFA500', '#A6D1F3']))])\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(title='Distribution of Observations by Season')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab40beb",
   "metadata": {},
   "source": [
    "This pie chart provides a visual representation of the distribution of observations across different seasons. Here are some insights we can get from this chart\n",
    "\n",
    "* Summer Dominance: With the largest segment at 28%, summer appears to be the season with the highest number of observations, indicating it might be the busiest or most active time of the year for sales.\n",
    "* Spring Significance: Spring is close behind summer, accounting for 27.3% of the observations, suggesting that it is also a significant time for sales.\n",
    "* Fall and Winter: Fall, represented by 23.8%, and winter, at 21%, have fewer observations, which could imply a decrease in  sales during these seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Observations by Quarter:\n",
    "year_counts = df['Quarter'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=year_counts.index,  # Quarter\n",
    "    y=year_counts.values, # Counts\n",
    "    text=year_counts.values,  # Text displayed on each bar\n",
    "    textposition='auto',  # Automatically position text\n",
    "    marker=dict(color=['#F08080', '#ADD8E6', '#90EE90', '#BC7FCD'])  # Color palette\n",
    ")])\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Count of Observations by Quarter',\n",
    "    xaxis=dict(title='Quarter'),\n",
    "    yaxis=dict(title='Count'),\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a94cb",
   "metadata": {},
   "source": [
    "The barplot graphically represents the counts of observations by quarter. Quarter 3 exhibits the highest count of observations, closely followed by Quarter 2 and Quarter 1, while Quarter 4 demonstrates the lowest count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ddad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts of observations by Quarter\n",
    "year_counts = df['Quarter'].value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "fig = go.Figure(data=[go.Pie(labels=year_counts.index, values=year_counts, textinfo='percent', marker=dict(colors=['#F08080', '#ADD8E6', '#90EE90', '#BC7FCD']))])\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(title='Distribution of Observations by Quarter')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d606d",
   "metadata": {},
   "source": [
    "The piechart illustrates the percentage distribution of observations across quarters. Notably, Quarter 3 has the highest percentage of observations at 28%, suggesting a potential increase in activity or sales during this period. Quarter 1 closely follows with 23.1%, while Quarters 2 and 4 exhibit lower percentages of observations at 27.3% and 21.7%, respectively.which helps to do strategic planning for resource allocation throughout the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Observations by Month:\n",
    "month_counts = df['Month_Name'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=month_counts.index,  # Months\n",
    "    y=month_counts.values,  # Counts\n",
    "    text=month_counts.values,  # Text displayed on each bar\n",
    "    textposition='auto',  # Automatically position text\n",
    ")])\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Count of Observations by Month',\n",
    "    xaxis=dict(title='Month'),\n",
    "    yaxis=dict(title='Count'),\n",
    ")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f473e",
   "metadata": {},
   "source": [
    "April and July emerge as peak sales months with 630 observations each, while March to October maintain steady activity around 585. However, November and January show notable declines to 360, possibly due to seasonal or post-holiday effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddbb458",
   "metadata": {},
   "source": [
    "### 7) Bivariate Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of Weekly Sales for Each Holiday Flag:\n",
    "df.groupby('Holiday_Flag')['Weekly_Sales'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4418e",
   "metadata": {},
   "source": [
    "**7.1 Mean of Weekly Sales for Each Holiday Flag:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98305d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean weekly sales for each holiday flag\n",
    "mean_sales = df.groupby('Holiday_Flag')['Weekly_Sales'].mean().reset_index()\n",
    "\n",
    "# Create the bar plot\n",
    "fig = px.bar(mean_sales, x='Holiday_Flag', y='Weekly_Sales', \n",
    "             title='Average Weekly Sales by Holiday Flag',\n",
    "             labels={'Holiday_Flag': 'Is Holiday', 'Weekly_Sales': 'Average Weekly Sales'})\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6a79f",
   "metadata": {},
   "source": [
    "* So from above plot we can easily deduce that higher average sales during holiday periods (1 on x-axis) compared to non-holidays (0), suggesting a positive impact of holidays on sales due to increased consumer spending. This insight enables businesses to strategize inventory and marketing efforts to capitalize on holiday shopping seasons, reflecting a common trend of sales spikes during holidays driven by promotions and festive shopping behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46602a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Holiday_Flag')['Weekly_Sales'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb76324",
   "metadata": {},
   "source": [
    "**7.2 Total Weekly Sales for Each Holiday Flag:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weekly sales for each holiday flag\n",
    "total_sales = df.groupby('Holiday_Flag')['Weekly_Sales'].sum().reset_index()\n",
    "# Create the bar plot\n",
    "fig = px.bar(total_sales, x='Holiday_Flag', y='Weekly_Sales', \n",
    "             title='Total Sales by Holidays',\n",
    "             labels={'holiday_flag': 'Is Holiday', 'weekly_sales': 'Total Sales'})\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6322a81",
   "metadata": {},
   "source": [
    "Total sales are higher during non-holiday periods, although holidays also see a sales spike. Businesses can use this data to strategize inventory and marketing, focusing on both holiday and non-holiday periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6459da",
   "metadata": {},
   "source": [
    "**7.3 Sum of Weekly Sales for Each Store:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of weekly sales for each store.\n",
    "total_sales_per_store = df.groupby('Store')['Weekly_Sales'].sum().reset_index()\n",
    "\n",
    "# Create the bar plot\n",
    "fig = px.bar(total_sales_per_store, x='Store', y='Weekly_Sales', \n",
    "             title='Total Sales in each Store',\n",
    "             labels={'Store': 'Store', 'Weekly_Sales': 'Total Sales'})\n",
    "\n",
    "# Set the figure size\n",
    "fig.update_layout(\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bf34e",
   "metadata": {},
   "source": [
    "* Top Performing Stores: Stores 20, 4, 14, and 13 are identified as the top performing stores based on total sales. These stores have demonstrated consistently high sales figures.\n",
    "\n",
    "* Worst Performing Stores: Stores 36, 5, 44, and 33 are identified as the worst performing stores based on total weekly sales. These stores have notably lower sales figures compared to others.\n",
    "\n",
    "So this bar graph shows a significant variation in weekly sales across different stores. Store No. 20 leads with the highest sales of 301,397,792, closely followed by Store No. 4 with sales of 299,543,953. In contrast, Store No. 33 has the lowest sales, amounting to 37,160,222. This suggests differing performance levels among the stores and potential areas for improvement, especially for those with lower sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9049e",
   "metadata": {},
   "source": [
    "**7.4) Scatter Plot Analysis**\n",
    "\n",
    "Because Weekly_Sales is influenced by Temperature, Fuel_Price, CPI, Unemployment, and Holiday_Flag, it's essential to draw scatter plots to understand the relationship between these columns and Weekly_Sales:\n",
    "\n",
    "* Temperature vs Weekly_Sales\n",
    "* Fuel_Price vs Weekly_Sales\n",
    "* CPI vs Weekly_Sales\n",
    "* Unemployment vs Weekly_Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb848ab1",
   "metadata": {},
   "source": [
    "**7.4.1 Temperature vs Weekly Sales:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='Temperature', y='Weekly_Sales', title='Temperature vs Weekly Sales', height = 400, width = 800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d4fcf",
   "metadata": {},
   "source": [
    "* There appears to be a moderate positive correlation between temperature and weekly sales, indicating that higher temperatures may lead to increased sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1082e6bb",
   "metadata": {},
   "source": [
    "**7.4.2 Scatter plot for Fuel_Price vs Weekly_Sales:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='Fuel_Price', y='Weekly_Sales', title='Fuel Price vs Weekly Sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc389e0",
   "metadata": {},
   "source": [
    "*  There is no clear trend between fuel price and weekly sales, suggesting that fuel price may not have a significant impact on sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e3b2d",
   "metadata": {},
   "source": [
    "**7.4.3 Scatter plot for CPI vs Weekly_Sales:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a69a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='CPI', y='Weekly_Sales', title='CPI vs Weekly Sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a0be5",
   "metadata": {},
   "source": [
    "* Consumer Price Index (CPI) has little impact on sales. Based on the distribution of typical consumer prices in the figure above, clients can be categorized into two groups: clients that pay from 120 and 150 are considered middle-class clients. consumers who pay between 180 and 230 are considered high-class consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f022fc8",
   "metadata": {},
   "source": [
    "**7.4.4 Scatter plot for Unemployment vs Weekly_Sales:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='Unemployment', y='Weekly_Sales', title='Unemployment vs Weekly Sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f5f19",
   "metadata": {},
   "source": [
    "* There appears to be a slight negative correlation between unemployment rate and weekly sales, indicating that lower unemployment rates may be associated with higher sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4206d00",
   "metadata": {},
   "source": [
    "**7.5 Analysis of Seasonal Trends in Weekly Sales**\n",
    "\n",
    "Identifying seasonal trends in weekly sales involves analyzing the data to observe patterns that repeat over specific periods, such as weeks, months, or quarters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f731a94",
   "metadata": {},
   "source": [
    "**7.5.1 Total sales in each year:**\n",
    "\n",
    "In this analysis, we examine the total sales in each year to identify any significant trends or fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e445c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by year and calculate total sales\n",
    "sales_by_year = df.groupby('Year')['Weekly_Sales'].sum().reset_index()\n",
    "\n",
    "# Create bar plot\n",
    "fig = go.Figure(data=go.Bar(\n",
    "    x=sales_by_year['Year'],\n",
    "    y=sales_by_year['Weekly_Sales'],\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Total Sales in each Year',\n",
    "    xaxis=dict(title='Year'),\n",
    "    yaxis=dict(title='Total Sales'),\n",
    "    width=750,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total sales in each year\n",
    "\n",
    "# Group data by year and calculate total sales\n",
    "sales_by_year = df.groupby('Year')['Weekly_Sales'].sum().reset_index()\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=sales_by_year['Year'],\n",
    "    y=sales_by_year['Weekly_Sales'],\n",
    "    mode='lines+markers',\n",
    "    marker=dict(color='blue', size=8),\n",
    "    line=dict(color='blue', width=2),\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Total Sales in each Year',\n",
    "    xaxis=dict(title='Year'),\n",
    "    yaxis=dict(title='Total Sales'),\n",
    "    width=750,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435be916",
   "metadata": {},
   "source": [
    "* In 2011, total sales peaked at $2.4482 billion, but there was a significant drop in 2012. This suggests that certain factors, potentially specific events, marketing campaigns, or economic conditions, might have influenced the sales performance. \n",
    "* It’s crucial to investigate these variables to understand the decline in 2012 and strategize effectively for future sales growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c685ae3d",
   "metadata": {},
   "source": [
    "**7.5.2 Total Sales in each Season:**\n",
    "\n",
    "This analysis examines the total sales in each season to understand the seasonal trends in sales performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Sales in each Season\n",
    "# Group data by season and calculate total sales\n",
    "sales_by_season = df.groupby('Season')['Weekly_Sales'].sum().reset_index()\n",
    "\n",
    "# Define color palette for each season\n",
    "colors = ['#FA6E1B', '#008000', '#FFA500', '#A6D1F3']\n",
    "\n",
    "# Create bar plot\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=sales_by_season['Season'],\n",
    "    y=sales_by_season['Weekly_Sales'],\n",
    "    marker_color=colors\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Total Sales in each Season',\n",
    "    xaxis=dict(title='Season'),\n",
    "    yaxis=dict(title='Total Sales'),\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21397d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by season and calculate total sales\n",
    "sales_by_season = df.groupby('Season')['Weekly_Sales'].sum().reset_index()\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure(go.Scatter(\n",
    "    x=sales_by_season['Season'],\n",
    "    y=sales_by_season['Weekly_Sales'],\n",
    "    mode='lines+markers',\n",
    "    marker=dict(color='blue', size=8),\n",
    "    line=dict(color='blue', width=2),\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Total Sales in each Season',\n",
    "    xaxis=dict(title='Season'),\n",
    "    yaxis=dict(title='Total Sales'),\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af960d",
   "metadata": {},
   "source": [
    "* So this graph indicates that Spring and Summer seasons experience the highest sales, which could be attributed to seasonal shopping habits, such as purchasing for holidays or outdoor activities. \n",
    "* Despite slight variations, total sales appear relatively consistent across all seasons, suggesting a stable customer base and steady demand throughout the year.\n",
    "* The lower sales in Fall and Winter could prompt Walmart to consider marketing strategies or promotions to boost sales during these seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac086f",
   "metadata": {},
   "source": [
    "**7.5.3 Total Sales in each Quarter:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Quarter and calculate total sales\n",
    "sales_by_month = df.groupby('Quarter')['Weekly_Sales'].sum().reset_index()\n",
    "\n",
    "# Create bar plot\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=sales_by_month['Quarter'],\n",
    "    y=sales_by_month['Weekly_Sales']\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Total Sales in each Quarter',\n",
    "    xaxis=dict(title='Quarter'),\n",
    "    yaxis=dict(title='Total Sales'),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0261bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Group data by quarter and calculate total sales\n",
    "sales_by_quarter = df.groupby('Quarter')['Weekly_Sales'].sum().reset_index()\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure(go.Scatter(\n",
    "    x=sales_by_quarter['Quarter'],\n",
    "    y=sales_by_quarter['Weekly_Sales'],\n",
    "    mode='lines+markers',\n",
    "    marker=dict(color='blue', size=8),\n",
    "    line=dict(color='blue', width=2),\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Total Sales in each Quarter',\n",
    "    xaxis=dict(title='Quarter'),\n",
    "    yaxis=dict(title='Total Sales'),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ffd2f",
   "metadata": {},
   "source": [
    "* This bar graph shows that sales in Quarters 2 and 3 are the highest, suggesting a mid-year peak in shopping activity, possibly due to summer holidays and back-to-school shopping. \n",
    "* Quarters 1 and 4 have lower sales, with Quarter 1 being the lowest, which may reflect post-holiday spending reductions and end-of-year budget constraints. \n",
    "* These insights are helpful in optimizing inventory and marketing strategies for different times of the year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847347c",
   "metadata": {},
   "source": [
    "**7.5.4 Total Sales in each Week:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a69ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_week = df.groupby('Week')['Weekly_Sales'].sum().sort_values(ascending=False)\n",
    "grp_week\n",
    "\n",
    "\n",
    "# Create a bar plot\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=grp_week.index,\n",
    "    y=grp_week.values,\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Total Sales in each Week',\n",
    "    xaxis=dict(title='Week'),\n",
    "    yaxis=dict(title='Total Sales'),\n",
    "    bargap=0.2,  # Set the gap between bars\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Group data by week and calculate total sales\n",
    "grp_week = df.groupby('Week')['Weekly_Sales'].sum().sort_index()\n",
    "\n",
    "# Create a line plot\n",
    "fig = go.Figure(go.Scatter(\n",
    "    x=grp_week.index,\n",
    "    y=grp_week.values,\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='blue', width=2),\n",
    "    marker=dict(color='blue', size=8),\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Total Sales in each Week',\n",
    "    xaxis=dict(title='Week'),\n",
    "    yaxis=dict(title='Total Sales'),\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812b3eb",
   "metadata": {},
   "source": [
    "The graph shows that Walmart’s weekly sales start strong, dip sharply around week 10, stabilize with minor fluctuations until week 40, and then exhibit significant volatility with sharp declines and spikes towards week 50. This pattern could reflect various external factors such as holidays, promotions, or seasonal changes affecting consumer behavior and sales trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22980ef",
   "metadata": {},
   "source": [
    "**7.5.5 Visualize Weekly Sales Over Time:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Bar(\n",
    "    x=df['Date'],\n",
    "    y=df['Weekly_Sales'],\n",
    "    marker_color='skyblue'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Weekly Sales Over Time',\n",
    "    xaxis=dict(title='Date', tickangle=45),\n",
    "    yaxis=dict(title='Weekly Sales'),\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize weekly sales over time\n",
    "# Create a line plot\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df['Date'], y=df['Weekly_Sales'],\n",
    "                    mode='lines',\n",
    "                    name='Weekly Sales'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Weekly Sales Over Time',\n",
    "                   xaxis_title='Date',\n",
    "                   yaxis_title='Weekly Sales',\n",
    "                   xaxis=dict(tickangle=45),\n",
    "                   yaxis=dict(gridcolor='lightgray'),\n",
    "                   showlegend=True)\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275cb3e",
   "metadata": {},
   "source": [
    "According to line plit we can say that from February 2010 to October 2012 shows that weekly sales have been relatively stable, with two significant spikes in sales occurring around January 2011 and January 2012. These spikes likely correspond to seasonal events or promotions that drove higher sales volumes. The consistent fluctuation between approximately 30M and 50M for most of the time suggests a steady demand, with occasional increases potentially tied to specific high-traffic events or holidays. This data can be crucial for planning inventory and sales strategies to capitalize on peak times and maintain a steady supply during regular demand periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b753c",
   "metadata": {},
   "source": [
    "**7.5.6 Weekly_sales over the years:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table\n",
    "weekly_sales = df.pivot_table(values='Weekly_Sales', columns='Year', index='Week')\n",
    "\n",
    "# Create a line plot for each year\n",
    "fig = go.Figure()\n",
    "for year in weekly_sales.columns:\n",
    "    fig.add_trace(go.Scatter(x=weekly_sales.index, y=weekly_sales[year], mode='lines', name=str(year)))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Weekly Sales Over the Years',\n",
    "    xaxis=dict(title='Week'),\n",
    "    yaxis=dict(title='Weekly Sales'),\n",
    "    legend=dict(title='Year'),\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890dd1d4",
   "metadata": {},
   "source": [
    "Conclusion:Total sales for all years in week 51 are the highest from any week, with 157,929,657$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c547695",
   "metadata": {},
   "source": [
    "**7.5.7 Total Sales for each Season in each Year:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14714011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by year and season and calculate total sales\n",
    "sales_by_year_season = df.groupby(['Year', 'Season'])['Weekly_Sales'].sum().reset_index()\n",
    "\n",
    "# Create a bar plot\n",
    "fig = px.bar(sales_by_year_season, x='Year', y='Weekly_Sales', color='Season', barmode='group',\n",
    "             title='Total Sales for each Season in each Year',\n",
    "             labels={'Year': 'Year', 'Weekly_Sales': 'Total Sales', 'Season': 'Season'})\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1911bc9",
   "metadata": {},
   "source": [
    "**Seasonal Sales Trends:** The graph shows that sales are highest in the Fall season each year, which could be due to back-to-school shopping or holiday sales events.          \n",
    "**Spring Sales Dip:** There was a noticeable drop in sales during the Spring of 2011, which may require further investigation to understand the cause.        \n",
    "**Consistent Winter Sales:** Sales during Winter have been consistent over the three years, indicating stable consumer behavior during this season.            \n",
    "**Growing Summer Sales:** There is an upward trend in Summer sales from 2010 to 2012, suggesting increasing consumer activity or successful marketing strategies in that period.           \n",
    "\n",
    "These insights can help in understanding consumer behavior and planning inventory and marketing strategies accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2d38b",
   "metadata": {},
   "source": [
    "**7.5.8 Total Sales for each Month in each year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by year and month and calculate total sales\n",
    "sales_by_year_month = df.groupby(['Year', 'Month_Name'])['Weekly_Sales'].sum().reset_index()\n",
    "\n",
    "# Create a bar plot\n",
    "fig = px.bar(sales_by_year_month, x='Year', y='Weekly_Sales', color='Month_Name', barmode='group',\n",
    "             title='Total Sales for each Month in each Year',\n",
    "             labels={'Year': 'Year', 'Weekly_Sales': 'Total Sales', 'Month_Name': 'Month'})\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a1327",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "* Monthly Sales Trends: The graph shows trends in total sales for each month over the years 2010, 2011, and 2012, highlighting which months had higher sales and allowing comparison of annual growth or decline.\n",
    "* Consistent High-Performing Months: April and December consistently show higher total sales compared to other months across all three years, suggesting these months may have key events or holidays that drive sales.\n",
    "* Sales Variability: The fluctuating monthly sales indicate that certain factors, possibly including seasonal changes, holidays, or promotions, significantly impact sales.\n",
    "* No Clear Growth Pattern: The absence of a consistent pattern of growth or decline suggests that sales are influenced by a variety of factors, rather than a steady market change.\n",
    "These insights can inform strategic decisions for inventory management, marketing campaigns, and sales forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e83f33",
   "metadata": {},
   "source": [
    "### 8) Multivariate Analysis:\n",
    "Now we are going to understand the relationship between all the different columns numerically to check how they correlate with the weekly sales in order to confirm the inferences we have gathered from the above EDA study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10314ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculate correlation matrix for numeric columns only\n",
    "correlation_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "# Create heatmap for all columns\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                    z=correlation_matrix, \n",
    "                    x=correlation_matrix.columns,\n",
    "                    y=correlation_matrix.columns,\n",
    "                    colorscale='RdYlBu',  \n",
    "                    ))\n",
    "\n",
    "fig.update_layout(title='Correlation Heatmap')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e334cc",
   "metadata": {},
   "source": [
    "The correlation matrix offers valuable insights into how different variables may influence weekly sales at Walmart. Here’s a summary of the insights:\n",
    "\n",
    "* Date and Fuel Price: There’s a strong positive correlation (0.77), suggesting that as time progresses, fuel prices also tend to increase.\n",
    "* Weekly Sales and Store: A negative correlation (-0.34) indicates that higher store numbers might be associated with lower weekly sales, which could suggest a pattern in store performance based on their numbering.\n",
    "* Weekly Sales and Unemployment: The negative correlation (-0.074) suggests that higher unemployment rates might be associated with slightly lower weekly sales.\n",
    "* CPI: The Consumer Price Index doesn’t show significant correlations with Holiday_Flag or Temperature, indicating that CPI’s impact on weekly sales might be independent of these factors.\n",
    "\n",
    "These insights can help Walmart understand the dynamics affecting sales and make informed decisions on inventory management, pricing, and promotions. However, it’s important to remember that correlation does not imply causation, and further analysis would be needed to establish direct relationships between these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b47dc",
   "metadata": {},
   "source": [
    "### 9)Data Correlation\n",
    "\n",
    "Correlation: A measure of the extent of interdependence between variables.\n",
    "\n",
    "Pearson Correlation: The Pearson Correlation measures the linear dependence between two variables X and Y.\n",
    "\n",
    "* The resulting coefficient is a value between -1 and 1 inclusive, where:\n",
    "\n",
    "* 1: Perfect positive linear correlation. \n",
    "* 0: No linear correlation, the two variables most likely do not affect each other. \n",
    "* -1: Perfect negative linear correlation.\n",
    "<br>\n",
    "\n",
    "* P-value: What is this P-value? The P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant.\n",
    "\n",
    "    * By convention, when the\n",
    "    * p-value is < 0.001: we say there is strong evidence that the correlation is significant.\n",
    "    * p-value is < 0.05: there is moderate evidence that the correlation is significant.\n",
    "    * p-value is < 0.1: there is weak evidence that the correlation is significant.\n",
    "    * p-value is > 0.1: there is no evidence that the correlation is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7378e",
   "metadata": {},
   "source": [
    "**9.1 Pearson Correlation Coefficient and P-value of 'Fuel_Price' and 'Weekly_Sales':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc76954",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Let's calculate the Pearson Correlation Coefficient and P-value of 'fuel_price' and 'weekly_sales':\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assuming you have a DataFrame 'data' with columns 'fuel_price' and 'weekly_sales'\n",
    "pearson_coef, p_value = pearsonr(df['Fuel_Price'], df['Weekly_Sales'])\n",
    "print(\"The Pearson Correlation Coefficient is\", pearson_coef, \"with a P-value of P =\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82338ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot with regression line\n",
    "fig = px.scatter(df, x='Fuel_Price', y='Weekly_Sales', trendline='ols', \n",
    "                 title='Weekly Sales vs. Fuel Price', \n",
    "                 labels={'fuel_price': 'Fuel Price', 'weekly_sales': 'Weekly Sales'})\n",
    "\n",
    "# Update trendline color\n",
    "fig.data[1].line.color = 'red'\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd96a2",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Since the p-value is > 0.1, the correlation between fuel price and weekly sales is not statistically significant.\n",
    "Fuel price does not seem like a good predictor of the weekly sales at all since the regression line is close to horizontal. Therefore, it's not a reliable variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb2681",
   "metadata": {},
   "source": [
    "**9.2 Unemployment vs. Weekly Sales:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c0b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef, p_value = pearsonr(df['Unemployment'], df['Weekly_Sales'])\n",
    "print(\"The Pearson Correlation Coefficient is\", pearson_coef, \"with a P-value of P =\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d602a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot with regression line\n",
    "fig = px.scatter(df, x='Unemployment', y='Weekly_Sales', trendline='ols', \n",
    "                 title='Weekly Sales vs. Unemployment Rate', \n",
    "                 labels={'unemployment': 'Unemployment Rate', 'weekly_sales': 'Weekly Sales'})\n",
    "\n",
    "# Update trendline color\n",
    "fig.data[1].line.color = 'red'\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c84479",
   "metadata": {},
   "source": [
    "Conclusion:-\n",
    "\n",
    "Since the p-value is < 0.001, the correlation between unemployment and weekly sales is strong evidence that the correlation is significant.\n",
    "Unemployment seems like a good predictor of the weekly sales, The higher the unemployment rate, the lower the weekly sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff09507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPI vs. Weekly Sales\n",
    "pearson_coef, p_value = pearsonr(df['CPI'], df['Weekly_Sales'])\n",
    "print(\"The Pearson Correlation Coefficient is\", pearson_coef, \"with a P-value of P =\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf6d9f",
   "metadata": {},
   "source": [
    "**9.3 CPI vs. Weekly Sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d99eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot with regression line\n",
    "fig = px.scatter(df, x='CPI', y='Weekly_Sales', trendline='ols', \n",
    "                 title='Weekly Sales vs. Consumer Price Index', \n",
    "                 labels={'cpi': 'Consumer Price Index', 'weekly_sales': 'Weekly Sales'})\n",
    "\n",
    "# Update trendline color\n",
    "fig.data[1].line.color = 'red'\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ed81b",
   "metadata": {},
   "source": [
    "Conclusion:Since the p-value is < 0.001, the correlation between CPI and weekly sales is strong evidence that the correlation is significant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6172517",
   "metadata": {},
   "source": [
    "**9.4 Temperature vs. Weekly Sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eea52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef, p_value = pearsonr(df['Temperature'], df['Weekly_Sales'])\n",
    "print(\"The Pearson Correlation Coefficient is\", pearson_coef, \"with a P-value of P =\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c2ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot with regression line\n",
    "fig = px.scatter(df, x='Temperature', y='Weekly_Sales', trendline='ols', \n",
    "                 title='Weekly Sales vs. Temperature', \n",
    "                 labels={'temperature': 'Temperature', 'weekly_sales': 'Weekly Sales'})\n",
    "\n",
    "# Update trendline color\n",
    "fig.data[1].line.color = 'red'\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f1311",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "* Since the p-value is < 0.05, the correlation between temperature and weekly sales is moderate evidence that the correlation is significant.\n",
    "* Temperature seems like a good predictor of the weekly sales, The higher the Temperature rate, the lower the weekly sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b555070",
   "metadata": {},
   "source": [
    "**9.5 Calculate correlation coefficients between Weekly_Sales and other numerical variables to assess the strength and direction of relationships.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dda23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns for correlation analysis\n",
    "numerical_columns = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df[numerical_columns].corr()\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                    z=correlation_matrix.values, \n",
    "                    x=numerical_columns,\n",
    "                    y=numerical_columns,\n",
    "                    colorscale='RdYlBu',  \n",
    "                    ))\n",
    "\n",
    "fig.update_layout(title='Correlation Heatmap')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36192b27",
   "metadata": {},
   "source": [
    "**9.5.1 Correlations with weekly sales:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']].corr()['Weekly_Sales'].sort_values(ascending = False)\n",
    "corr = corr.to_frame()\n",
    "corr.style.background_gradient(cmap=\"RdYlBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ed36d",
   "metadata": {},
   "source": [
    "* Fuel Price and Weekly Sales: There’s a slight positive correlation, indicating that as fuel prices increase, weekly sales might also show a slight increase.\n",
    "* Temperature and Weekly Sales: There’s a negative correlation, suggesting that higher temperatures might lead to a decrease in weekly sales.\n",
    "* CPI and Weekly Sales: The negative correlation here implies that as the Consumer Price Index rises, weekly sales might decrease.\n",
    "* Unemployment and Weekly Sales: Similarly, a negative correlation indicates that higher unemployment rates might be associated with lower weekly sales.\n",
    "\n",
    "In summary, the dataset indicates that higher fuel prices have a very slight positive effect on weekly sales, while increased temperatures, a higher CPI, and higher unemployment rates might negatively impact weekly sales. However, all these effects are relatively small according to the correlation coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51071fdc",
   "metadata": {},
   "source": [
    "<h2 style = \"color : red\" >Project Questions</h2>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b88ff9",
   "metadata": {},
   "source": [
    "### 1. You are provided with the weekly sales data for their various outlets. Use statistical analysis, EDA, outlier analysis, and handle the missing values to come up with various insights that can give them a clear perspective on the following:\n",
    "**a. If the weekly sales are affected by the unemployment rate, if yes - which stores\n",
    "are suffering the most?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3afcd2c",
   "metadata": {},
   "source": [
    "Let just pick those who have less Weekly Sales. As it will be too lenghty to analyze 45 stores.\n",
    "So what we do is we take 12 stores with least weekly_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the sum of weekly sales in descending order and reset the index\n",
    "sorted_sales = df.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False).reset_index()\n",
    "last_10_stores = sorted_sales.tail(12)['Store'].tolist()\n",
    "print(last_10_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1040792",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame containing the necessary data\n",
    "\n",
    "# Initialize subplot grid\n",
    "fig = make_subplots(rows=4, cols=3, subplot_titles=[\n",
    "    f\"Store {store_num}\" for store_num in [42, 9, 29, 16, 37, 30, 3, 38, 36, 5, 44, 33]\n",
    "])\n",
    "\n",
    "# Iterate over each store and add a trace for the scatter plot\n",
    "for i, store_num in enumerate([42, 9, 29, 16, 37, 30, 3, 38, 36, 5, 44, 33], start=1):\n",
    "    store_data = df[df['Store'] == store_num]\n",
    "    fig.add_trace(go.Scatter(x=store_data['Unemployment'], y=store_data['Weekly_Sales'],\n",
    "                             mode='markers', name=f'Store {store_num}'),\n",
    "                  row=(i - 1) // 3 + 1, col=(i - 1) % 3 + 1)\n",
    "\n",
    "# Update layout to add titles\n",
    "fig.update_layout(title_text=\"Weekly Sales vs Unemployment for Various Stores\",\n",
    "                  height=1200, width=1000)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d5a83",
   "metadata": {},
   "source": [
    "* Now here we can clearly see that \"Store 44\" and \"Store 42\" is suffering the most by weekly sales\n",
    "* These observations suggest that Store 42 and store 44 may require specific attention or strategies to mitigate the impact of unemployment rate fluctuations on its weekly sales performance. Meanwhile, for other stores, while there may still be some impact, it appears to be less severe or pronounced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a2836",
   "metadata": {},
   "source": [
    "**b. If the weekly sales show a seasonal trend, when and what could be the reason?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1882ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_date = df[['Date','Weekly_Sales']]\n",
    "sales_date.set_index('Date',inplace=True)\n",
    "sales_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fb212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create traces for each column in sales_date\n",
    "traces = []\n",
    "for column in sales_date.columns:\n",
    "    trace = go.Scatter(x=sales_date.index, y=sales_date[column], mode='lines', name=column)\n",
    "    traces.append(trace)\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=traces)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title=\"Weekly Sales over Time\",\n",
    "                  xaxis_title=\"Date\",\n",
    "                  yaxis_title=\"Weekly Sales\",\n",
    "                  legend_title=\"Stores\",\n",
    "                  width=1000, height=500)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9afe37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the sales by holiday season.\n",
    "\n",
    "# Create trace\n",
    "trace = go.Scatter(x=df['Date'], y=df['Holiday_Flag'], mode='lines', name='Holiday Flag')\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=trace)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title=\"Holiday Flag over Time\",\n",
    "                  xaxis_title=\"Date\",\n",
    "                  yaxis_title=\"Holiday Flag\",\n",
    "                  width=1000, height=500)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7bc5e",
   "metadata": {},
   "source": [
    "* We can clearly observe a seasonality component in weekly sales, where sales tend to fluctuate cyclically throughout the year. However, towards the end of the year, there is a noticeable exponential increase in sales.\n",
    "* This significant increase in sales towards the end of the year is primarily attributed to the holiday season, during which consumer spending typically rises.\n",
    "* Notably, this spike in sales coincides with the holiday season, particularly during Christmas and New Year in North America, where Walmart holds significant prominence.\n",
    "* It's common for brands, including Walmart, to offer various promotions and discounts during the holiday season. These promotional activities likely contribute to the sudden surge in sales observed at the end of the year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea6354",
   "metadata": {},
   "source": [
    "**c. Does temperature affect the weekly sales in any manner?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation coefficient between temperature and weekly sales\n",
    "correlation = df['Temperature'].corr(df['Weekly_Sales'])\n",
    "print(f'Correlation coefficient between temperature and weekly sales: {correlation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming data is your DataFrame containing the necessary data\n",
    "# Assuming sales_date is your DataFrame containing sales data\n",
    "\n",
    "# Create line plot for 'Temperature'\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(x=df['Date'], y=df['Temperature'], mode='lines', name='Temperature'))\n",
    "fig1.update_layout(title=\"Temperature over Time\",\n",
    "                   xaxis_title=\"Date\",\n",
    "                   yaxis_title=\"Temperature\",\n",
    "                   width=1000, height=500)\n",
    "\n",
    "# Create line plot for 'Weekly Sales'\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(x=sales_date.index, y=sales_date['Weekly_Sales'], mode='lines', name='Weekly Sales'))\n",
    "fig2.update_layout(title=\"Weekly Sales over Time\",\n",
    "                   xaxis_title=\"Date\",\n",
    "                   yaxis_title=\"Weekly Sales\",\n",
    "                   width=1000, height=500)\n",
    "\n",
    "# Show the plots\n",
    "fig1.show()\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75975192",
   "metadata": {},
   "source": [
    "The only noted effect that can be seen is again of the holiday season. Holiday season are marked with winters and snow, that increases the needed clothing and stuff. Other than this there is no such clear trend of shopping related with temprature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e26a94",
   "metadata": {},
   "source": [
    "**d. How is the Consumer Price index affecting the weekly sales of various stores?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eac42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation coefficient between temperature and weekly sales\n",
    "correlation = df['CPI'].corr(df['Weekly_Sales'])\n",
    "print(f'Correlation coefficient between CPI and weekly sales: {correlation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed46bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming data is your DataFrame containing the necessary data\n",
    "# Assuming sales_date is your DataFrame containing sales data\n",
    "\n",
    "# Create line plot for 'CPI'\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(x=df['Date'], y=df['CPI'], mode='lines', name='CPI'))\n",
    "fig1.update_layout(title=\"Consumer Price Index (CPI) over Time\",\n",
    "                   xaxis_title=\"Date\",\n",
    "                   yaxis_title=\"CPI\",\n",
    "                   width=1000, height=500)\n",
    "fig1.show()\n",
    "\n",
    "# Create line plot for 'Weekly Sales'\n",
    "fig2 = go.Figure()\n",
    "for column in sales_date.columns:\n",
    "    fig2.add_trace(go.Scatter(x=sales_date.index, y=sales_date[column], mode='lines', name=column))\n",
    "fig2.update_layout(title=\"Weekly Sales over Time\",\n",
    "                   xaxis_title=\"Date\",\n",
    "                   yaxis_title=\"Weekly Sales\",\n",
    "                   width=1000, height=500)\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693e52e",
   "metadata": {},
   "source": [
    "Although there is inflation over time represented by increasing CPI over the time period. There is no upward or downward trend followed by weekly sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e7d57",
   "metadata": {},
   "source": [
    "**e. Top performing stores according to the historical data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc81a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sales data for each store\n",
    "total_sales = df.groupby('Store')['Weekly_Sales'].sum().reset_index()\n",
    "\n",
    "# Rank stores based on total sales\n",
    "total_sales_sorted = total_sales.sort_values(by='Weekly_Sales', ascending=False)\n",
    "\n",
    "# Top performing stores\n",
    "total_sales_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed335c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(total_sales_sorted, x='Store', y='Weekly_Sales', \n",
    "             labels={'Weekly_Sales': 'Total Sales'}, \n",
    "             title='Top Performing Stores')\n",
    "fig.update_layout(xaxis_title='Store', yaxis_title='Total Sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d8be1",
   "metadata": {},
   "source": [
    "Top performing 5 stores are --> store number 20, 4, 14, 13, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f6da8",
   "metadata": {},
   "source": [
    "**f. The worst performing store, and how significant is the difference between the\n",
    "highest and lowest performing stores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae22992",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_sorted.tail(5).sort_values('Weekly_Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb1a254",
   "metadata": {},
   "source": [
    "Worst performing stores are 33, 44, 5, 36, 38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34914e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming Store20 and Store33 are your DataFrames containing the necessary data\n",
    "\n",
    "# Create traces for Store20 and Store33\n",
    "trace1 = go.Scatter(x=df[df['Store']==20]['Date'], y=df[df['Store']==20]['Weekly_Sales'], mode='lines', name='Store 20', line=dict(color='turquoise'))\n",
    "trace2 = go.Scatter(x=df[df['Store']==33]['Date'], y=df[df['Store']==33]['Weekly_Sales'], mode='lines', name='Store 33', line=dict(color='chocolate'))\n",
    "\n",
    "# Create subplot figure\n",
    "fig = go.Figure([trace1, trace2])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title=\"Weekly Sales Comparison between best and worst performing store.png\",\n",
    "                  xaxis_title=\"Date\",\n",
    "                  yaxis_title=\"Weekly Sales\",\n",
    "                  width=1000, height=600)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c1ff5",
   "metadata": {},
   "source": [
    "Significant difference in highest and lowest performing store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9849eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_sorted.set_index('Store', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "(total_sales_sorted.loc[33][0]/total_sales_sorted.loc[20][0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f073747",
   "metadata": {},
   "source": [
    "Lowest performing store's sales only accounts for 12% of sales done by top performing store on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b17ec",
   "metadata": {},
   "source": [
    "## Step:5 Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e890d82",
   "metadata": {},
   "source": [
    "#### Que:2 Use predictive modeling techniques to forecast the sales for each store for the next 12 weeks.\n",
    "\n",
    "The choice of a model for continuous prediction depends on several factors, including the nature of your data, the presence of seasonality or trends, and the specific requirements of your problem.\n",
    "\n",
    "Choosing the right model often involves experimentation and evaluation. It is good practice to try multiple models and compare their performance using appropriate evaluation metrics, such as R-squared, mean squared error (MSE), root mean squared error (RMSE), or mean absolute error (MAE), on a validation dataset. Additionally, consider the interpretability of the model and computational efficiency, especially for large-scale applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e8dfba",
   "metadata": {},
   "source": [
    "I have decided to compare Four ML models namely:\n",
    "\n",
    "* Linear Regression\n",
    "* Decision Tree\n",
    "* Random Forest Regressor\n",
    "* XGBOOST\n",
    "\n",
    "We will compare these four models and pick the best perfoming model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cdcac",
   "metadata": {},
   "source": [
    "### 5.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e847c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03261739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target variable\n",
    "X = df[['Store', 'Holiday_Flag', 'CPI', 'Day', 'Month', 'Year']]\n",
    "y = df['Weekly_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3dd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8934aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e42ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients of the Linear Regression model\n",
    "coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': linear_model.coef_})\n",
    "coefficients = coefficients.sort_values(by='Coefficient', ascending=False)\n",
    "print(\"Linear Regression Coefficients:\")\n",
    "print()\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa0909",
   "metadata": {},
   "source": [
    "These coefficients represent the relationship between each feature and the target variable, assuming all other features remain constant. Here’s what they mean:\n",
    "\n",
    "* Holiday_Flag (66532.229815): When it’s a holiday, sales are expected to increase by approximately 66,532 units, holding all other features constant.\n",
    "* Month (13726.510155): With each increase in month number (e.g., from January to February), sales are expected to increase by approximately 13,726 units.\n",
    "Year (4488.717371): For each year, sales are expected to increase by approximately 4,488 units.\n",
    "* Day (-1667.560022): With each passing day, sales are expected to decrease by approximately 1,667 units.\n",
    "* CPI (-2088.070923): As the Consumer Price Index (CPI) increases by one unit, sales are expected to decrease by approximately 2,088 units.\n",
    "* Store (-15719.395592): This might indicate that as the store number (perhaps as a proxy for location or other factors) increases, sales are expected to decrease by approximately 15,719 units.\n",
    "\n",
    "Positive coefficients indicate a positive relationship with the target variable, while negative coefficients indicate a negative relationship. It’s important to note that these are statistical estimates and actual results may vary based on additional factors not included in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sales on the test set\n",
    "y_pred_linear = linear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"Linear Regression R2 Score: \", r2_score(y_test, y_pred_linear))\n",
    "print(\"Linear Regression MSE Score: \", mean_squared_error(y_test, y_pred_linear))\n",
    "print(\"Linear Regression RMSE : \", sqrt(mean_squared_error(y_test, y_pred_linear)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34911577",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create traces for actual and predicted sales\n",
    "actual_trace = go.Scatter(x=df['Date'][len(df)-len(y_test):], y=y_test, mode='lines', name='Actual Weekly Sales', line=dict(color='blue'))\n",
    "predicted_trace = go.Scatter(x=df['Date'][len(df)-len(y_test):], y=y_pred_linear, mode='lines', name='Predicted Weekly Sales (Linear Regression)', line=dict(color='red', dash='dash'))\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure([actual_trace, predicted_trace])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='Weekly Sales', title='Actual vs Predicted Weekly Sales (Linear Regression)')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b465432c",
   "metadata": {},
   "source": [
    "### 5.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67fc45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create an instance of DecisionTreeRegressor\n",
    "dtree = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the model to the training data\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e51578",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ed30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"Decision Tree R2 Score: \", r2_score(y_test, y_pred_dt))\n",
    "print(\"Decision Tree MSE Score: \", mean_squared_error(y_test, y_pred_dt))\n",
    "print(\"Decision Tree RMSE : \", sqrt(mean_squared_error(y_test, y_pred_dt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces for actual and predicted sales\n",
    "actual_trace = go.Scatter(x=df['Date'].iloc[-len(y_test):], y=y_test, mode='lines', name='Actual Weekly Sales', line=dict(color='blue'))\n",
    "predicted_trace = go.Scatter(x=df['Date'].iloc[-len(y_test):], y=y_pred_dt, mode='lines', name='Predicted Weekly Sales (Decision Tree)', line=dict(color='red', dash='dash'))\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure([actual_trace, predicted_trace])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='Weekly Sales', title='Actual vs Predicted Weekly Sales (Decision Tree)')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3b06f",
   "metadata": {},
   "source": [
    "### 5.3 Random Forest Regressor¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9308fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create a Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7017ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sales on the test set\n",
    "y_pred_rf = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce298cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"Random Forest Regressor R2 Score: \", r2_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Regressor MSE Score: \", mean_squared_error(y_test, y_pred_rf))\n",
    "print(\"Random Forest Regressor RMSE : \", sqrt(mean_squared_error(y_test, y_pred_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces for actual and predicted sales\n",
    "actual_trace = go.Scatter(x=df['Date'].iloc[-len(y_test):], y=y_test, mode='lines', name='Actual Weekly Sales', line=dict(color='blue'))\n",
    "predicted_trace = go.Scatter(x=df['Date'].iloc[-len(y_test):], y=y_pred_rf, mode='lines', name='Predicted Weekly Sales (Random Forest)', line=dict(color='red', dash='dash'))\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure([actual_trace, predicted_trace])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='Weekly Sales', title='Actual vs Predicted Weekly Sales (Random Forest)')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10c9e5",
   "metadata": {},
   "source": [
    "### 5.4 XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Create an instance of XGBRegressor\n",
    "xg = XGBRegressor()\n",
    "\n",
    "# Fit the model to the training data\n",
    "xg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb900544",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xg = xg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f248ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"XG Boost R2 Score: \", r2_score(y_test, y_pred_xg))\n",
    "print(\"XG Boost MSE Score: \", mean_squared_error(y_test, y_pred_xg))\n",
    "print(\"XG Boost RMSE : \", sqrt(mean_squared_error(y_test, y_pred_xg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afad18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create traces for actual and predicted sales\n",
    "actual_trace = go.Scatter(x=df['Date'].iloc[-len(y_test):], y=y_test, mode='lines', name='Actual Weekly Sales', line=dict(color='blue'))\n",
    "predicted_trace = go.Scatter(x=df['Date'].iloc[-len(y_test):], y=y_pred_xg, mode='lines', name='Predicted Weekly Sales (XG Boost)', line=dict(color='red', dash='dash'))\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure([actual_trace, predicted_trace])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='Weekly Sales', title='Actual vs Predicted Weekly Sales (XG Boost)')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e99d59",
   "metadata": {},
   "source": [
    "### 5.5 Summary of Applied Predictive Modeling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7813e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression R2 Score: \", r2_score(y_test, y_pred_linear))\n",
    "print(\"Linear Regression MSE Score: \", mean_squared_error(y_test, y_pred_linear))\n",
    "print(\"Linear Regression RMSE : \", sqrt(mean_squared_error(y_test, y_pred_linear)))\n",
    "print(\"Decision Tree R2 Score: \", r2_score(y_test, y_pred_dt))\n",
    "print(\"Decision Tree MSE Score: \", mean_squared_error(y_test, y_pred_dt))\n",
    "print(\"Decision Tree RMSE : \", sqrt(mean_squared_error(y_test, y_pred_dt)))\n",
    "print(\"Random Forest Regressor R2 Score: \", r2_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Regressor MSE Score: \", mean_squared_error(y_test, y_pred_rf))\n",
    "print(\"Random Forest Regressor RMSE : \", sqrt(mean_squared_error(y_test, y_pred_rf)))\n",
    "print(\"XG Boost R2 Score: \", r2_score(y_test, y_pred_xg))\n",
    "print(\"XG Boost MSE Score: \", mean_squared_error(y_test, y_pred_xg))\n",
    "print(\"XG Boost RMSE : \", sqrt(mean_squared_error(y_test, y_pred_xg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a168989",
   "metadata": {},
   "source": [
    "The XG Boost model outperforms the others in terms of R2 Score, MSE Score, and RMSE. It shows the highest R2 Score, indicating better fit to the data, and the lowest MSE and RMSE, indicating lower prediction errors.\n",
    "\n",
    "Therefore, we should choose the XG Boost model for sales forecasting because it provides the most accurate predictions compared to the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7616916",
   "metadata": {},
   "source": [
    "**5.5.1 Predicting weekly_sales for 1st row using XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afa082",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb959c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg.predict(pd.DataFrame([[1,0,211.096358,5,2,2010]]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780198b",
   "metadata": {},
   "source": [
    "Here, We're using XGBoost to predict the weekly sales for the first row, and it's performing exceptionally well in making accurate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb73e5b",
   "metadata": {},
   "source": [
    "## 5.6 Sales forecasting for the next 12 weeks using XGBoost for all 45 stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4d4dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_week_date = pd.Timestamp('2012-10-26')\n",
    "next_sunday = latest_week_date + pd.DateOffset(days=(6 - latest_week_date.dayofweek) + 1)  # Get the next Sunday\n",
    "date_range = pd.date_range(start=next_sunday, periods=12, freq='W-FRI')  # Start from Friday instead of Sunday\n",
    "date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d27d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the last week in your dataset is the most recent week\n",
    "latest_week_date = df['Date'].max()\n",
    "\n",
    "# Creating a DataFrame with dates for the next 12 weeks\n",
    "next_12_weeks = pd.date_range(start=latest_week_date, periods=12, freq='W')\n",
    "\n",
    "latest_week_date = pd.Timestamp('2012-10-26')\n",
    "next_sunday = latest_week_date + pd.DateOffset(days=(6 - latest_week_date.dayofweek) + 1)  # Get the next Sunday\n",
    "next_12_weeks = pd.date_range(start=next_sunday, periods=12, freq='W-FRI')  # Start from Friday instead of Sunday\n",
    "\n",
    "\n",
    "# Creating a DataFrame to hold predictions\n",
    "forecast_df = pd.DataFrame(index=next_12_weeks, columns=range(1, 46))\n",
    "\n",
    "# Making predictions for next 12 weeks for each store\n",
    "for store_num in range(1, 46):\n",
    "    store = df[df['Store'] == 1][['Store', 'Holiday_Flag', 'CPI', 'Weekly_Sales', 'Date']]\n",
    "    future_data = pd.DataFrame({\n",
    "        'Store': [store_num] * 12,\n",
    "        'Holiday_Flag': [0] * 12,  # Assuming no holidays in the next 12 weeks\n",
    "        'CPI': [store.CPI.mean()] * 12,\n",
    "        'Day': next_12_weeks.day,\n",
    "        'Month': next_12_weeks.month,\n",
    "        'Year': next_12_weeks.year,\n",
    "        'Date': next_12_weeks\n",
    "    })\n",
    "\n",
    "    # Making predictions for next 12 weeks for the current store\n",
    "    future_predictions = xg.predict(future_data.drop(columns=['Date']))\n",
    "\n",
    "    # Storing predictions in the forecast DataFrame\n",
    "    forecast_df[store_num] = future_predictions\n",
    "\n",
    "# Round the forecasted sales values to two decimal points\n",
    "forecast_df = forecast_df.round(2)\n",
    "\n",
    "# Transpose the forecast DataFrame for better visualization\n",
    "forecast_df = forecast_df.transpose()\n",
    "\n",
    "# Rename the index to 'Store'\n",
    "forecast_df.index.name = 'Store'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e956e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the forecast DataFrame for better visualization\n",
    "forecast_df = forecast_df.transpose()\n",
    "\n",
    "# Round the forecasted sales values to two decimal points\n",
    "forecast_df = forecast_df.round(2)\n",
    "\n",
    "pd.DataFrame(forecast_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a0d78",
   "metadata": {},
   "source": [
    "Above matrix is the forecast for all the stores for next 12 weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4198f",
   "metadata": {},
   "source": [
    "## 5.7 Actual vs Forecasted Graph for 5 Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e2dbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    # Selecting store Individually \n",
    "    store = df[df['Store'] == i][['Store', 'Holiday_Flag', 'CPI', 'Weekly_Sales', 'Date']]\n",
    "\n",
    "    # Extract day,mont,year component from the 'Date' column\n",
    "    store['Day'] = store['Date'].dt.day\n",
    "    store['Month'] = store['Date'].dt.month\n",
    "    store['Year'] = store['Date'].dt.year\n",
    "\n",
    "    # Only date column from individually selected store\n",
    "    only_date = pd.DataFrame(store['Date'])\n",
    "    \n",
    "    # Only date column from individually selected store\n",
    "    only_weekly_sales = pd.DataFrame(store['Weekly_Sales'])\n",
    "\n",
    "    # Drop Date and Weekly_Sales column from store dataframe\n",
    "    store = store.drop(columns=['Date','Weekly_Sales'])\n",
    "\n",
    "    # Getting forecasting dates for next 12 weeks\n",
    "    latest_week_date = pd.Timestamp('2012-10-26')\n",
    "    next_sunday = latest_week_date + pd.DateOffset(days=(6 - latest_week_date.dayofweek) + 1)  # Get the next Sunday\n",
    "    date_range = pd.date_range(start=next_sunday, periods=12, freq='W-FRI')  # Start from Friday instead of Sunday\n",
    "    next_date = pd.DataFrame(date_range)\n",
    "    next_date = next_date.rename(columns={0: 'Date'})\n",
    "\n",
    "    # Concatenate actual+next 12 weeks date\n",
    "    old_plus_new_date = pd.concat([only_date,next_date], axis = 0)\n",
    "\n",
    "    # Inputs for predicting next 12 weeks weekly_sales\n",
    "    data = {'Store': [i] * 12,\n",
    "            'Holiday_Flag': [0] * 12,\n",
    "            'CPI': [store.CPI.mean()] * 12,\n",
    "            'Day': next_12_weeks.day,\n",
    "            'Month': next_12_weeks.month,\n",
    "            'Year': next_12_weeks.year}\n",
    "\n",
    "    # Make dataframe considering above Data \n",
    "    forecast_df = pd.DataFrame(data)\n",
    "    forecast_df\n",
    "\n",
    "    # Concatenate Individual store plus 'Inputs for predicting next 12 weeks weekly_sales'\n",
    "    whole = pd.concat([store,forecast_df,], axis = 0)\n",
    "\n",
    "    # Predicting weekly_sales for historical plus future \n",
    "    all = xg.predict(whole)\n",
    "\n",
    "    # Create traces for actual and predicted sales\n",
    "    actual_trace = go.Scatter(x=df['Date'].iloc[-len(store):], y=only_weekly_sales.Weekly_Sales, mode='lines', name='Actual Weekly Sales', line=dict(color='blue'))\n",
    "    predicted_trace = go.Scatter(x=old_plus_new_date['Date'].iloc[-len(old_plus_new_date):], y=all, mode='lines', name='Predicted Weekly Sales (XG Boost)', line=dict(color='red', dash='dash'))\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure([actual_trace, predicted_trace])\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(xaxis_title='Date', yaxis_title='Weekly Sales', title=f'Actual vs Predicted Weekly Sales for store {i} (XG Boost)')\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6729f35",
   "metadata": {},
   "source": [
    "So, here we have plotted graphs for the actual versus predicted weekly sales for 5 stores. If we want, we can generate similar plots for other stores by adjusting the specified range accordingly.\n",
    "\n",
    "The variations between the actual and predicted weekly sales could indeed be influenced by factors like CPI and holiday flags. In our prediction model, we've assumed that CPI remains constant at the mean value for each store and that there are no holidays (holiday_flag = 0) for the next 12 weeks. However, in reality, there might be fluctuations in CPI and holidays such as Christmas and New Year during the upcoming weeks, which can affect sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4032c",
   "metadata": {},
   "source": [
    "## Step:6  Model Saving and Loading for Scalability and Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec17d8",
   "metadata": {},
   "source": [
    "#### 6.1 Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Saving the model\n",
    "model_file_path = '../models/wallmart.pkl'\n",
    "\n",
    "# Save the model to the specified file path\n",
    "joblib.dump(xg, model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994f1b7",
   "metadata": {},
   "source": [
    "#### 6.2 Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525571da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Loading the model\n",
    "loaded_model = joblib.load(model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57aa6f6",
   "metadata": {},
   "source": [
    "#### 6.3 Perform predictions using the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff47220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Perform predictions using the loaded model\n",
    "predictions = loaded_model.predict(pd.DataFrame([[1,0,211.096358,5,2,2010]]))[0]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11832005",
   "metadata": {},
   "source": [
    "## Step: 7 Conclusion\n",
    "\n",
    "In conclusion, this project aimed to forecast sales for multiple stores over the next 12 weeks\n",
    "using predictive modeling techniques. Through comprehensive data analysis, model\n",
    "development, and evaluation, valuable insights have been uncovered that can inform\n",
    "strategic decision-making and drive business growth.\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "`1. Impact of Predictor Variables:` The analysis revealed the significant influence of\n",
    "predictor variables such as holiday flags, consumer price index (CPI), and\n",
    "unemployment rates on weekly sales. Understanding these factors is crucial for\n",
    "anticipating sales trends and optimizing resource allocation.                 \n",
    "`2. Seasonal Trends and Patterns:` Seasonal fluctuations in sales were observed,\n",
    "highlighting the importance of accounting for temporal patterns in forecasting\n",
    "models. By incorporating seasonal adjustments, businesses can better align their\n",
    "operations with demand fluctuations throughout the year.              \n",
    "`3. Model Performance:` Evaluation of various predictive modeling techniques\n",
    "demonstrated the superiority of advanced algorithms such as Random Forest and\n",
    "XGBOOST in forecasting sales accurately. These models outperformed simpler\n",
    "techniques and provided more robust predictions.            \n",
    "`4. Business Insights for Decision-making:` The insights generated from sales forecasting\n",
    "can guide strategic decision-making in areas such as inventory management,\n",
    "marketing strategies, and resource allocation. By leveraging predictive analytics,\n",
    "businesses can optimize operations, reduce costs, and improve customer satisfaction            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
